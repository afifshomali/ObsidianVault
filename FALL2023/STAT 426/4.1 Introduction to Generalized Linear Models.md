---
tags:
  - STAT426
  - STAT426/Midterm2
---
---
![[Pasted image 20230929214023.png]]

$\alpha$ is the intercept, the rest of terms are the same as in previous stat classes

We want to model independent observations of a response variable in terms of $p$ explanatory variables.
We can represent all the variables as numbers by using Dummy variables for categorical variables which ensures we can convert it into something that can be used in a linear model.

![[Pasted image 20230929214508.png]]
We re-write the the pmf or density function of a distribution in the form of the random component. Where the $\theta_i$ is the parameter.
The linear predictor is just a linear combination of the predictors $x_{i1}$ to $x_{ip}$ .
 ![[Link function]] 
 The Identity link maps values to itself, it means the same as the identity function: $g(x) = x$
 then $g(E[Y]) = g(\mu) = \mu$ 
 
![[Canonical link]]

Since $Y_i$ is derived from a natural exponential family, its distribution is completely determined by its mean $\mu_i$ .

In particular, $var(Y_i)$ is a function of $\mu_i$ .

![[Bernoulli distribution#Binary Regression using Bernoulli]]
 ![[Logistic regression]] 
 ![[Poisson distribution#Poisson Regression]]
 ![[Loglinear Model]]
 To Fit the GLM we use [[Maximum likelihood estimation]] or the Least Squares solution which is $\hat{\beta} = (X^{T}X)^{-1} X^T y$ 
 ![[Pasted image 20230929221140.png]]
We have to assume that the $y_i$ s are independent for this to work, otherwise we could have the product to represent the probability of the joint distribution of $y_i$ s.

