---
tags:
  - CS447
---
---
![[{034E39FA-4616-4E3E-87DF-A1D5B040FC0B}.png]]
![[{700DFFAF-BEF2-4289-AD43-8B9C6C70EA5C}.png]]
self-attention switches the For each decoder to for each encoder 
## Self Attention
---
![[{1828CFAE-EE03-4848-A65F-46AA85984290}.png]]
![[{36B6F759-21A5-4C99-BC6C-B51514730182}.png]]
![[{9072458D-3C07-4BA5-959C-AA5AC04D2EBB}.png]]
## Transformer
---
![[{96EFB216-B69F-4ABD-872F-C774098B7C75}.png]]
![[{B93256D8-17AD-4ECA-9987-0E3438F76400}.png]]
LayerNorm(x + Sublayer(x))
![[{A806E211-1735-4BAC-BC85-6011A1C01B0D}.png]]

## Multi Head Attention
---
![[{E3A5D9E5-700D-48E2-8245-C1EECEDE8A76}.png]]
![[{3D9A5024-859E-473C-91FA-9389EBE08CA0}.png]]
Multihead(Q,K,V) = Concat($head_1, ..., head_h)$ $W$ 
![[{E6F27017-368A-4312-9931-3D1805063FED}.png]]
![[{D7DC7D46-3C0E-4F74-BE4D-E1553F6866AD}.png]]
![[{5DDF17BE-5A1C-4D13-8254-D79D6533A0EA}.png]]
